{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ConvAftermath"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ConvAftermath(nn.Module):\n",
    "    \"\"\"\n",
    "    use_bias:数据添加偏置\n",
    "    use_scale:数据放缩\n",
    "    \"\"\"\n",
    "    def __init__(self, in_channels, out_channels, use_bias=True, use_scale=True, norm=None, act=None):\n",
    "        super(ConvAftermath, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.use_bias = use_bias\n",
    "        self.use_scale = use_scale\n",
    "        self.norm = norm\n",
    "        self.act = act\n",
    "        self.b = None\n",
    "        self.s = None\n",
    "\n",
    "    def forward(self, input):\n",
    "        net = input\n",
    "        if self.use_bias and self.b is not None:\n",
    "            net = net + self.use_bias\n",
    "        if self.use_scale and self.s is not None:\n",
    "            net = net * self.s\n",
    "        if self.norm is not None:\n",
    "            net = self.norm(net)\n",
    "        if self.act is not None:\n",
    "            net = self.act(net)\n",
    "        return net"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "疑问：\n",
    "self.s=None?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conv_ReflectPad"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "padding_mode:zeros（常量填充）（默认0填充）、reflect（反射填充）、replicate（复制填充）、circular（循环填充）。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Conv2D_ReflectPad(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride=1, use_bias=True, use_scale=True, norm=None,\n",
    "                 act=None,\n",
    "                 padding='same', padding_algorithm=\"reflect\"):\n",
    "        super(Conv2D_ReflectPad, self).__init__()\n",
    "        self.padding = padding\n",
    "        self.padding_algorithm = padding_algorithm\n",
    "        self.act = act\n",
    "        self.norm = norm\n",
    "        self.use_scale = use_scale\n",
    "        self.use_bias = use_bias\n",
    "        self.strides = stride\n",
    "        self.kernel_size = kernel_size\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.atrous_rate = 1\n",
    "        if padding == 'same':\n",
    "            self.padding = self.kernel_size // 2 if self.strides == 1 else 0\n",
    "            self.pad_flag = True\n",
    "        else:\n",
    "            self.padding = padding\n",
    "\n",
    "        self.conv = nn.Conv2d(in_channels=self.in_channels,\n",
    "                              out_channels=self.out_channels,\n",
    "                              kernel_size=self.kernel_size,\n",
    "                              stride=self.strides,\n",
    "                              bias=False,\n",
    "                              padding=self.padding,\n",
    "                              padding_mode=self.padding_algorithm)\n",
    "        self.conv_aftermath = ConvAftermath(in_channels=self.out_channels,\n",
    "                                            out_channels=self.out_channels,\n",
    "                                            use_bias=self.use_bias,\n",
    "                                            use_scale=self.use_scale,\n",
    "                                            norm=self.norm,\n",
    "                                            act=self.act)\n",
    "\n",
    "    def forward(self, input):\n",
    "        x = self.conv(input)\n",
    "        y = self.conv_aftermath(x)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考：\n",
    "+ padding_mode: https://blog.csdn.net/weixin_42211626/article/details/122542323"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## DCB"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\DCB.png />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrousBlockPad2(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, stride, use_bias, use_scale, activation,\n",
    "                 needs_projection = False,atrousBlock=[1, 2, 4, 8]):\n",
    "        super(AtrousBlockPad2, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.kernel_size = kernel_size\n",
    "        self.stride = stride\n",
    "        self.use_bias = use_bias\n",
    "        self.use_scale = use_scale\n",
    "        self.activation = activation\n",
    "        self.atrousBlocks = atrousBlock\n",
    "        # self.needs_projection = self.stride > 1\n",
    "        self.dims_match = self.in_channels != self.out_channels\n",
    "        # self.needs_projection = self.needs_projection or self.dims_match\n",
    "        self.needs_projection = needs_projection\n",
    "\n",
    "        if self.needs_projection:\n",
    "            self.projection = Conv2D_ReflectPad(in_channels=self.in_channels,\n",
    "                                                out_channels=self.out_channels,\n",
    "                                                kernel_size=1,\n",
    "                                                stride=self.stride,\n",
    "                                                use_bias=self.use_bias,\n",
    "                                                act=self.activation)\n",
    "        self.atrous_layers = []\n",
    "\n",
    "        for i in range(4):\n",
    "            self.atrous_layers.append(AtrousConv2D_ReflectPad(in_channels=self.out_channels,\n",
    "                                                              out_channels=int(self.out_channels / 2),\n",
    "                                                              kernel_size=self.kernel_size,\n",
    "                                                              stride=self.stride,\n",
    "                                                              dilation=atrousBlock[i],\n",
    "                                                              use_bias=self.use_bias,\n",
    "                                                              use_scale=self.use_scale,\n",
    "                                                              act=self.activation))\n",
    "        self.atrous_layers = nn.Sequential(*self.atrous_layers)\n",
    "\n",
    "        self.conv1 = Conv2D_ReflectPad(in_channels=self.out_channels * 2,\n",
    "                                       out_channels=self.out_channels,\n",
    "                                       kernel_size=self.kernel_size,\n",
    "                                       stride=self.stride,\n",
    "                                       use_bias=self.use_bias,\n",
    "                                       use_scale=self.use_scale,\n",
    "                                       act=self.activation\n",
    "                                       )\n",
    "\n",
    "    def forward(self, input):\n",
    "        if self.needs_projection:\n",
    "            input = self.projection(input)\n",
    "\n",
    "        x1 = self.atrous_layers[0](input)\n",
    "        x2 = self.atrous_layers[1](input)\n",
    "        x3 = self.atrous_layers[2](input)\n",
    "        x4 = self.atrous_layers[3](input)\n",
    "\n",
    "        x = torch.cat((x1, x2, x3, x4), 1)\n",
    "        x5 = self.conv1(x)\n",
    "\n",
    "        return input + x5"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 小波变换模块"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\WRRM.png />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt_init(x):\n",
    "    x01 = x[:, :, 0::2, :] / 2\n",
    "    x02 = x[:, :, 1::2, :] / 2\n",
    "    x1 = x01[:, :, :, 0::2]\n",
    "    x2 = x02[:, :, :, 0::2]\n",
    "    x3 = x01[:, :, :, 1::2]\n",
    "    x4 = x02[:, :, :, 1::2]\n",
    "    x_LL = x1 + x2 + x3 + x4\n",
    "    x_HL = -x1 - x2 + x3 + x4\n",
    "    x_LH = -x1 + x2 - x3 + x4\n",
    "    x_HH = x1 - x2 - x3 + x4\n",
    "    return torch.cat((x_LL, x_HL, x_LH, x_HH), 0)\n",
    "\n",
    "\n",
    "# 使用哈尔 haar 小波变换来实现二维离散小波\n",
    "def iwt_init(x):\n",
    "    r = 2\n",
    "    in_batch, in_channel, in_height, in_width = x.size()\n",
    "    #print([in_batch, in_channel, in_height, in_width])\n",
    "    out_batch, out_channel, out_height, out_width = int(in_batch / r ** 2), int(\n",
    "        in_channel), r * in_height, r * in_width\n",
    "    x1 = x[0:out_batch, :, :, :] / 2\n",
    "    x2 = x[out_batch:out_batch * 2, :, :, :] / 2\n",
    "    x3 = x[out_batch * 2:out_batch * 3, :, :, :] / 2\n",
    "    x4 = x[out_batch * 3:out_batch * 4, :, :, :] / 2\n",
    "\n",
    "    h = torch.zeros([out_batch, out_channel, out_height,\n",
    "                     out_width]).float().cuda()\n",
    "\n",
    "    h[:, :, 0::2, 0::2] = x1 - x2 - x3 + x4\n",
    "    h[:, :, 1::2, 0::2] = x1 - x2 + x3 - x4\n",
    "    h[:, :, 0::2, 1::2] = x1 + x2 - x3 - x4\n",
    "    h[:, :, 1::2, 1::2] = x1 + x2 + x3 + x4\n",
    "\n",
    "    return h\n",
    "\n",
    "# 二维离散小波\n",
    "class DWT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(DWT, self).__init__()\n",
    "        self.requires_grad = False  # 信号处理，非卷积运算，不需要进行梯度求导\n",
    "\n",
    "    def forward(self, x):\n",
    "        return dwt_init(x)\n",
    "\n",
    "\n",
    "# 逆向二维离散小波\n",
    "class IWT(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(IWT, self).__init__()\n",
    "        self.requires_grad = False\n",
    "\n",
    "    def forward(self, x):\n",
    "        return iwt_init(x)\n",
    "class SRCNN(nn.Module):\n",
    "    def __init__(self, num_channels, out_channels):\n",
    "        super(SRCNN, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(num_channels, 64, kernel_size=9, padding=9 // 2)\n",
    "        self.conv2 = nn.Conv2d(64, 32, kernel_size=5, padding=5 // 2)\n",
    "        self.conv3 = nn.Conv2d(32, out_channels, kernel_size=5, padding=5 // 2)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.DWT = DWT()\n",
    "        self.IDWT = IWT()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.DWT(x)\n",
    "        x = self.relu(self.conv1(x))\n",
    "        x = self.relu(self.conv2(x))\n",
    "        x = self.conv3(x)\n",
    "        x = self.IDWT(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "小波变换：为了获取高频的纹理信息。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## efficientattention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<div align=center>\n",
    "<img src=.\\img\\efficientattention.png width=\"1000\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class EfficientAttention(nn.Module):\n",
    "    def __init__(self, in_channels, key_channels, head_count, value_channels):\n",
    "        super().__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.key_channels = key_channels\n",
    "        self.head_count = head_count\n",
    "        self.value_channels = value_channels\n",
    "\n",
    "        self.keys = nn.Conv2d(in_channels, key_channels, 1)\n",
    "        self.queries = nn.Conv2d(in_channels, key_channels, 1)\n",
    "        self.values = nn.Conv2d(in_channels, value_channels, 1)\n",
    "        self.reprojection = nn.Conv2d(value_channels, in_channels, 1)\n",
    "\n",
    "    def forward(self, input_):\n",
    "        n, _, h, w = input_.size()\n",
    "        keys = self.keys(input_).reshape((n, self.key_channels, h * w))\n",
    "        queries = self.queries(input_).reshape(n, self.key_channels, h * w)\n",
    "        values = self.values(input_).reshape((n, self.value_channels, h * w))\n",
    "        head_key_channels = self.key_channels // self.head_count\n",
    "        head_value_channels = self.value_channels // self.head_count\n",
    "\n",
    "        attended_values = []\n",
    "        for i in range(self.head_count):\n",
    "            key = f.softmax(keys[\n",
    "                            :,\n",
    "                            i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                            :\n",
    "                            ], dim=2)\n",
    "            query = f.softmax(queries[\n",
    "                              :,\n",
    "                              i * head_key_channels: (i + 1) * head_key_channels,\n",
    "                              :\n",
    "                              ], dim=1)\n",
    "            value = values[\n",
    "                :,\n",
    "                i * head_value_channels: (i + 1) * head_value_channels,\n",
    "                :\n",
    "            ]\n",
    "            context = key @ value.transpose(1, 2)\n",
    "            attended_value = (\n",
    "                context.transpose(1, 2) @ query\n",
    "            ).reshape(n, head_value_channels, h, w)\n",
    "            attended_values.append(attended_value)\n",
    "\n",
    "        aggregated_values = torch.cat(attended_values, dim=1)\n",
    "        reprojected_value = self.reprojection(aggregated_values)\n",
    "        attention = reprojected_value + input_\n",
    "\n",
    "        return attention"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "高效注意力是一种注意力机制，它极大地优化了内存和计算效率，同时保留了与传统的点积注意力完全相同的表现力。上图比较了这两种类型的注意力。优点：1、使用更少的资源实现相同的准确率。2、相同的资源达到更高的性能。"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考文献：\n",
    "+ https://github.com/cmsflash/efficient-attention\n",
    "+ https://www.bilibili.com/video/BV1Gt4y1Y7E3\n",
    "+ Efficient Attention: Attention with Linear Complexities"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ContextBlock"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\Contextblock.png width=\"500\" />\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class ContextBlock(nn.Module):\n",
    "\n",
    "    def __init__(self,\n",
    "                 inplanes,\n",
    "                 ratio,\n",
    "                 pooling_type='att',\n",
    "                 fusion_types=('channel_add', )):\n",
    "        super(ContextBlock, self).__init__()\n",
    "        assert pooling_type in ['avg', 'att']\n",
    "        assert isinstance(fusion_types, (list, tuple))\n",
    "        valid_fusion_types = ['channel_add', 'channel_mul']\n",
    "        assert all([f in valid_fusion_types for f in fusion_types])\n",
    "        assert len(fusion_types) > 0, 'at least one fusion should be used'\n",
    "        self.inplanes = inplanes\n",
    "        self.ratio = ratio\n",
    "        self.planes = int(inplanes * ratio)\n",
    "        self.pooling_type = pooling_type\n",
    "        self.fusion_types = fusion_types\n",
    "        if pooling_type == 'att':\n",
    "            self.conv_mask = nn.Conv2d(inplanes, 1, kernel_size=1)\n",
    "            self.softmax = nn.Softmax(dim=2)\n",
    "        else:\n",
    "            self.avg_pool = nn.AdaptiveAvgPool2d(1)\n",
    "        if 'channel_add' in fusion_types:\n",
    "            self.channel_add_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                # nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_add_conv = None\n",
    "        if 'channel_mul' in fusion_types:\n",
    "            self.channel_mul_conv = nn.Sequential(\n",
    "                nn.Conv2d(self.inplanes, self.planes, kernel_size=1),\n",
    "                # nn.LayerNorm([self.planes, 1, 1]),\n",
    "                nn.ReLU(inplace=True),  # yapf: disable\n",
    "                nn.Conv2d(self.planes, self.inplanes, kernel_size=1))\n",
    "        else:\n",
    "            self.channel_mul_conv = None\n",
    "        self.reset_parameters()\n",
    "\n",
    "    def reset_parameters(self):\n",
    "        if self.pooling_type == 'att':\n",
    "            kaiming_init(self.conv_mask, mode='fan_in')\n",
    "            self.conv_mask.inited = True\n",
    "\n",
    "        if self.channel_add_conv is not None:\n",
    "            last_zero_init(self.channel_add_conv)\n",
    "        if self.channel_mul_conv is not None:\n",
    "            last_zero_init(self.channel_mul_conv)\n",
    "\n",
    "    def spatial_pool(self, x):\n",
    "        batch, channel, height, width = x.size()\n",
    "        if self.pooling_type == 'att':\n",
    "            input_x = x\n",
    "            # [N, C, H * W]\n",
    "            input_x = input_x.view(batch, channel, height * width)\n",
    "            # [N, 1, C, H * W]\n",
    "            input_x = input_x.unsqueeze(1)\n",
    "            # [N, 1, H, W]\n",
    "            context_mask = self.conv_mask(x)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = context_mask.view(batch, 1, height * width)\n",
    "            # [N, 1, H * W]\n",
    "            context_mask = self.softmax(context_mask)\n",
    "            # [N, 1, H * W, 1]\n",
    "            context_mask = context_mask.unsqueeze(-1)\n",
    "            # [N, 1, C, 1]\n",
    "            context = torch.matmul(input_x, context_mask)\n",
    "            # [N, C, 1, 1]\n",
    "            context = context.view(batch, channel, 1, 1)\n",
    "        else:\n",
    "            # [N, C, 1, 1]\n",
    "            context = self.avg_pool(x)\n",
    "\n",
    "        return context\n",
    "\n",
    "    def forward(self, x):\n",
    "        # [N, C, 1, 1]\n",
    "        context = self.spatial_pool(x)\n",
    "\n",
    "        out = x\n",
    "        if self.channel_mul_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_mul_term = torch.sigmoid(self.channel_mul_conv(context))\n",
    "            out = out * channel_mul_term\n",
    "        if self.channel_add_conv is not None:\n",
    "            # [N, C, 1, 1]\n",
    "            channel_add_term = self.channel_add_conv(context)\n",
    "            out = out + channel_add_term\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GCNet充分结合了Non-local全局上下文建模能力强和SENet省计算量的优点"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "参考:\n",
    "+ https://github.com/xvjiarui/GCNet\n",
    "+ GCNet: Non-Local Networks Meet Squeeze-Excitation Networks and Beyond"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## basic model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\netstructure.png />\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrousNet(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels, num_blocks=10, max_global_stride=8, pad_to_fit_global_stride=True,\n",
    "                 d_mult=16,\n",
    "                 activation=nn.ELU(alpha=1.0, inplace=True),\n",
    "                 atrousDim=[1, 2, 4, 8]):\n",
    "        super(AtrousNet, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_blocks = num_blocks\n",
    "        self.max_global_stride = max_global_stride\n",
    "        self.pad_to_fit_global_stride = pad_to_fit_global_stride\n",
    "        self.d_mult = d_mult\n",
    "        self.activation = activation\n",
    "\n",
    "        self.downsampling_layers = []\n",
    "        self.downsampling_layers.append(Conv2D_ReflectPad(in_channels=self.in_channels,\n",
    "                                                          out_channels=self.d_mult,\n",
    "                                                          kernel_size=7,\n",
    "                                                          stride=1,\n",
    "                                                          use_bias=True,\n",
    "                                                          use_scale=True,\n",
    "                                                          padding=\"same\",\n",
    "                                                          act=self.activation))\n",
    "        self.downsampling_layers.append(Conv2D_ReflectPad(in_channels=self.d_mult,\n",
    "                                                          out_channels=self.d_mult * 2,\n",
    "                                                          kernel_size=3,\n",
    "                                                          stride=2,\n",
    "                                                          use_scale=True,\n",
    "                                                          use_bias=True,\n",
    "                                                          padding=1,\n",
    "                                                          act=self.activation))\n",
    "        self.downsampling_layers = nn.Sequential(*self.downsampling_layers)\n",
    "\n",
    "        self.blocks = []\n",
    "        for x in range(num_blocks):\n",
    "            if x == 0:\n",
    "                self.blocks.append(AtrousBlockPad2(in_channels=self.d_mult * 2,\n",
    "                                                   out_channels=self.d_mult * 4,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   activation=self.activation,\n",
    "                                                   atrousBlock=atrousDim,\n",
    "                                                   needs_projection=True))\n",
    "            else:\n",
    "                self.blocks.append(AtrousBlockPad2(in_channels=self.d_mult * 4,\n",
    "                                                   out_channels=self.d_mult * 4,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   activation=self.activation,\n",
    "                                                   atrousBlock=atrousDim))\n",
    "\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.upsampling_layers = []\n",
    "        self.upsampling_layers.append(nn.ConvTranspose2d(in_channels=self.d_mult * 6,  # Error: should be 4\n",
    "                                                         out_channels=self.d_mult * 2,\n",
    "                                                         kernel_size=3,\n",
    "                                                         stride=2,\n",
    "                                                         padding=1,\n",
    "                                                         output_padding=1,\n",
    "                                                         bias=True))\n",
    "        self.upsampling_layers.append(nn.ELU(alpha=1.0, inplace=True))\n",
    "        self.upsampling_layers = nn.Sequential(*self.upsampling_layers)\n",
    "\n",
    "        self.output_layer = []\n",
    "        self.output_layer.append(Conv2D_ReflectPad(in_channels=self.d_mult * 3,\n",
    "                                                   out_channels=self.d_mult,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   padding='same',\n",
    "                                                   act=self.activation))\n",
    "        self.output_layer.append(Conv2D_ReflectPad(in_channels=self.d_mult,\n",
    "                                                   out_channels=self.out_channels,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   padding='same',\n",
    "                                                   act=None))\n",
    "        self.output_layer = nn.Sequential(*self.output_layer)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        downs = []\n",
    "        net = input_data\n",
    "        for x in range(len(self.downsampling_layers)):\n",
    "            net = self.downsampling_layers[x](net)\n",
    "            downs.append(net)\n",
    "\n",
    "        for x in range(len(self.blocks)):\n",
    "            net = self.blocks[x](net)\n",
    "\n",
    "        for x in range(len(self.upsampling_layers)):\n",
    "            idx = len(downs) - x - 1\n",
    "            net = torch.cat((net, downs[idx]), 1)\n",
    "            net = self.upsampling_layers[x](net)\n",
    "\n",
    "        for x in range(len(self.output_layer)):\n",
    "            net = self.output_layer[x](net)\n",
    "\n",
    "        return input_data + net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AtrousNet_SRCNN_tail(nn.Module):\n",
    "    \"\"\"\n",
    "    num_blocks: the num of DCB\n",
    "    max_global_stride: no use?\n",
    "    pad_to_fit_global_stride: no use?\n",
    "    d_mult: channel\n",
    "    efficientattention:\n",
    "    gcattention:\n",
    "    \"\"\"\n",
    "    def __init__(self,\n",
    "                in_channels,\n",
    "                out_channels,\n",
    "                num_blocks=10,\n",
    "                max_global_stride=8,\n",
    "                pad_to_fit_global_stride=True,\n",
    "                d_mult=32,\n",
    "                activation=nn.ELU(alpha=1.0, inplace=True),\n",
    "                atrousDim=[[1, 2, 4, 8],[1, 3, 5, 7]],\n",
    "                efficientattention=False,\n",
    "                gcattention=False,\n",
    "                ):\n",
    "        super(AtrousNet_SRCNN_tail, self).__init__()\n",
    "        self.in_channels = in_channels\n",
    "        self.out_channels = out_channels\n",
    "        self.num_blocks = num_blocks\n",
    "        self.max_global_stride = max_global_stride\n",
    "        self.pad_to_fit_global_stride = pad_to_fit_global_stride\n",
    "        self.d_mult = d_mult\n",
    "        self.activation = activation\n",
    "        self.efficientattention = efficientattention\n",
    "        self.gcattention = gcattention\n",
    "\n",
    "        self.downsampling_layers = []\n",
    "        self.downsampling_layers.append(Conv2D_ReflectPad(in_channels=self.in_channels,\n",
    "                                                          out_channels=self.d_mult,\n",
    "                                                          kernel_size=7,\n",
    "                                                          stride=1,\n",
    "                                                          use_bias=True,\n",
    "                                                          use_scale=True,\n",
    "                                                          padding=\"same\",\n",
    "                                                          act=self.activation))\n",
    "        self.downsampling_layers.append(Conv2D_ReflectPad(in_channels=self.d_mult,\n",
    "                                                          out_channels=self.d_mult * 2,\n",
    "                                                          kernel_size=3,\n",
    "                                                          stride=2,\n",
    "                                                          use_scale=True,\n",
    "                                                          use_bias=True,\n",
    "                                                          padding=1,\n",
    "                                                          act=self.activation))\n",
    "        self.downsampling_layers = nn.Sequential(*self.downsampling_layers)\n",
    "        self.SRCNN = SRCNN(self.d_mult*7, self.out_channels)\n",
    "        self.blocks = []\n",
    "        for x in range(num_blocks):\n",
    "            if x == 0:\n",
    "                self.blocks.append(AtrousBlockPad2(in_channels=self.d_mult * 2,\n",
    "                                                   out_channels=self.d_mult * 4,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   activation=self.activation,\n",
    "                                                   atrousBlock=atrousDim[0],\n",
    "                                                   needs_projection=True))\n",
    "            elif x != num_blocks-1:\n",
    "                self.blocks.append(AtrousBlockPad2(in_channels=self.d_mult * 4,\n",
    "                                                   out_channels=self.d_mult * 4,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   activation=self.activation,\n",
    "                                                   atrousBlock=atrousDim[0]))\n",
    "            else:\n",
    "                self.blocks.append(AtrousBlockPad2(in_channels=self.d_mult * 4,\n",
    "                                                   out_channels=self.d_mult * 4,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   activation=self.activation,\n",
    "                                                   atrousBlock=atrousDim[1]))\n",
    "\n",
    "        self.blocks = nn.Sequential(*self.blocks)\n",
    "\n",
    "        self.upsampling_layers = []\n",
    "        self.upsampling_layers.append(nn.Upsample(scale_factor = 2, mode='bilinear', align_corners=True))\n",
    "        self.upsampling_layers.append(nn.ELU(alpha=1.0, inplace=True))\n",
    "        self.upsampling_layers = nn.Sequential(*self.upsampling_layers)\n",
    "\n",
    "        self.output_layer = []\n",
    "        # modify output later from 3 * d_multi to 7 * multi\n",
    "        self.output_layer.append(Conv2D_ReflectPad(in_channels=self.d_mult * 7,\n",
    "                                                   out_channels=self.d_mult,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   padding='same',\n",
    "                                                   act=self.activation))\n",
    "        if self.efficientattention:\n",
    "            self.output_layer.append(EfficientAttention(in_channels=self.d_mult, key_channels=self.d_mult, head_count=4, value_channels=self.d_mult))\n",
    "        elif self.gcattention:\n",
    "            self.output_layer.append(ContextBlock(inplanes=self.d_mult, ratio=0.25))\n",
    "        self.output_layer.append(Conv2D_ReflectPad(in_channels=self.d_mult,\n",
    "                                                   out_channels=self.out_channels,\n",
    "                                                   kernel_size=3,\n",
    "                                                   stride=1,\n",
    "                                                   use_bias=True,\n",
    "                                                   use_scale=True,\n",
    "                                                   padding='same',\n",
    "                                                   act=None))\n",
    "        self.output_layer = nn.Sequential(*self.output_layer)\n",
    "\n",
    "    def forward(self, input_data):\n",
    "        downs = []\n",
    "        net = input_data\n",
    "        for x in range(len(self.downsampling_layers)):\n",
    "            net = self.downsampling_layers[x](net)\n",
    "            downs.append(net)\n",
    "\n",
    "        for x in range(len(self.blocks)):\n",
    "            net = self.blocks[x](net)\n",
    "\n",
    "        for x in range(len(self.upsampling_layers)):\n",
    "            idx = len(downs) - x - 1\n",
    "            net = torch.cat((net, downs[idx]), 1)\n",
    "            net = self.upsampling_layers[x](net)\n",
    "\n",
    "        SRCNN_net = self.SRCNN(net)\n",
    "        for x in range(len(self.output_layer)):\n",
    "            net = self.output_layer[x](net)\n",
    "\n",
    "        return input_data + net + SRCNN_net\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8e1e697be24aa51be46f7515f4d96c6005120fc689094ce96895b044c9b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
