{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "使用MLP架构取得SOTA的表现"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\FeedForward.png width=\"20%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class FeedForward(nn.Module):\n",
    "    def __init__(self, dim, hidden_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(dim, hidden_dim),\n",
    "            nn.GELU(),\n",
    "            nn.Dropout(dropout),\n",
    "            nn.Linear(hidden_dim, dim),\n",
    "            nn.Dropout(dropout)\n",
    "        )\n",
    "    def forward(self, x):\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\MixerBlock.png width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MixerBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, dim, num_patch, token_dim, channel_dim, dropout = 0.):\n",
    "        super().__init__()\n",
    "\n",
    "        self.token_mix = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Rearrange('b n d -> b d n'),\n",
    "            FeedForward(num_patch, token_dim, dropout),\n",
    "            Rearrange('b d n -> b n d')\n",
    "        )\n",
    "\n",
    "        self.channel_mix = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            FeedForward(dim, channel_dim, dropout),\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x = x + self.token_mix(x)\n",
    "\n",
    "        x = x + self.channel_mix(x)\n",
    "\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\model.png width=\"80%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPMixer(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, dim, num_classes, patch_size, image_size, depth, token_dim, channel_dim):\n",
    "        super().__init__()\n",
    "\n",
    "        assert image_size % patch_size == 0, 'Image dimensions must be divisible by the patch size.'\n",
    "        self.num_patch =  (image_size// patch_size) ** 2\n",
    "        self.to_patch_embedding = nn.Sequential(\n",
    "            nn.Conv2d(in_channels, dim, patch_size, patch_size),\n",
    "            Rearrange('b c h w -> b (h w) c'),\n",
    "        )\n",
    "\n",
    "        self.mixer_blocks = nn.ModuleList([])\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.mixer_blocks.append(MixerBlock(dim, self.num_patch, token_dim, channel_dim))\n",
    "\n",
    "        self.layer_norm = nn.LayerNorm(dim)\n",
    "\n",
    "        self.mlp_head = nn.Sequential(\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "\n",
    "        x = self.to_patch_embedding(x)\n",
    "\n",
    "        for mixer_block in self.mixer_blocks:\n",
    "            x = mixer_block(x)\n",
    "\n",
    "        x = self.layer_norm(x)\n",
    "\n",
    "        x = x.mean(dim=1)\n",
    "\n",
    "        return self.mlp_head(x)"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
