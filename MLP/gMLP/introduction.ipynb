{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "# functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def dropout_layers(layers, prob_survival):\n",
    "    if prob_survival == 1:\n",
    "        return layers\n",
    "\n",
    "    num_layers = len(layers)\n",
    "    to_drop = torch.zeros(num_layers).uniform_(0., 1.) > prob_survival\n",
    "\n",
    "    # make sure at least one layer makes it\n",
    "    if all(to_drop):\n",
    "        rand_index = randrange(num_layers)\n",
    "        to_drop[rand_index] = False\n",
    "\n",
    "    layers = [layer for (layer, drop) in zip(layers, to_drop) if not drop]\n",
    "    return layers\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_inner, causal = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim_inner ** -0.5\n",
    "        self.causal = causal\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim_in, dim_inner * 3, bias = False)\n",
    "        self.to_out = nn.Linear(dim_inner, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if self.causal:\n",
    "            mask = torch.ones(sim.shape[-2:], device = device).triu(1).bool()\n",
    "            sim.masked_fill_(mask[None, ...], -torch.finfo(q.dtype).max)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        return self.to_out(out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpatialGatingUnit\n",
    "<div align=center>\n",
    "<img src=./img/1.png>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGatingUnit(nn.Module):\n",
    "    def __init__(self, dim, dim_seq, attn_dim = None, causal = False):\n",
    "        super().__init__()\n",
    "        dim_out = dim // 2\n",
    "        self.causal = causal\n",
    "\n",
    "        self.norm = nn.LayerNorm(dim_out)\n",
    "        self.proj = nn.Conv1d(dim_seq, dim_seq, 1)\n",
    "        self.attn = Attention(dim, dim_out, attn_dim, causal) if exists(attn_dim) else None\n",
    "        nn.init.zeros_(self.proj.weight)\n",
    "        nn.init.constant_(self.proj.bias, 1.)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "\n",
    "        res, gate = x.chunk(2, dim = -1)\n",
    "        gate = self.norm(gate)\n",
    "\n",
    "        weight, bias = self.proj.weight, self.proj.bias\n",
    "        if self.causal:\n",
    "            mask = torch.ones(weight.shape[:2], device = device).triu_(1).bool()\n",
    "            weight = weight.masked_fill(mask[..., None], 0.)\n",
    "\n",
    "        gate = F.conv1d(gate, weight, bias)\n",
    "\n",
    "        if exists(self.attn):\n",
    "            gate += self.attn(x)\n",
    "        return gate * res"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gMLPVision(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        depth,\n",
    "        ff_mult = 4,\n",
    "        channels = 3,\n",
    "        attn_dim = None,\n",
    "        prob_survival = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (image_size % patch_size) == 0, 'image size must be divisible by the patch size'\n",
    "        dim_ff = dim * ff_mult\n",
    "        num_patches = (image_size // patch_size) ** 2\n",
    "\n",
    "        self.to_patch_embed = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1 = patch_size, p2 = patch_size),\n",
    "            nn.Linear(channels * patch_size ** 2, dim)\n",
    "        )\n",
    "\n",
    "        self.prob_survival = prob_survival\n",
    "\n",
    "        self.layers = nn.ModuleList([Residual(nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            nn.Linear(dim, dim_ff),\n",
    "            nn.GELU(),\n",
    "            SpatialGatingUnit(dim_ff, num_patches, attn_dim),\n",
    "            nn.Linear(dim_ff // 2, dim)\n",
    "        )) for i in range(depth)])\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Reduce('b n d -> b d', 'mean'),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embed(x)\n",
    "        layers = self.layers if not self.training else dropout_layers(self.layers, self.prob_survival)\n",
    "        x = nn.Sequential(*layers)(x)\n",
    "        return self.to_logits(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.6.2 ('pytorch')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.6.2"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "1be8e1e697be24aa51be46f7515f4d96c6005120fc689094ce96895b044c9b1b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
