{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from random import randrange\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch import nn, einsum\n",
    "\n",
    "from einops import rearrange, repeat\n",
    "from einops.layers.torch import Rearrange, Reduce\n",
    "\n",
    "# functions\n",
    "\n",
    "def exists(val):\n",
    "    return val is not None\n",
    "\n",
    "def pair(val):\n",
    "    return (val, val) if not isinstance(val, tuple) else val\n",
    "\n",
    "def dropout_layers(layers, prob_survival):\n",
    "    if prob_survival == 1:\n",
    "        return layers\n",
    "\n",
    "    num_layers = len(layers)\n",
    "    to_drop = torch.zeros(num_layers).uniform_(0., 1.) > prob_survival\n",
    "\n",
    "    # make sure at least one layer makes it\n",
    "    if all(to_drop):\n",
    "        rand_index = randrange(num_layers)\n",
    "        to_drop[rand_index] = False\n",
    "\n",
    "    layers = [layer for (layer, drop) in zip(layers, to_drop) if not drop]\n",
    "    return layers\n",
    "\n",
    "\n",
    "    if amount == 0:\n",
    "        return t\n",
    "    return F.pad(t, (0, 0, amount, -amount), value = 0.)\n",
    "\n",
    "# helper classes\n",
    "\n",
    "class Residual(nn.Module):\n",
    "    def __init__(self, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.fn(x) + x\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super().__init__()\n",
    "        self.fn = fn\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        x = self.norm(x)\n",
    "        return self.fn(x, **kwargs)\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_inner, causal = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim_inner ** -0.5\n",
    "        self.causal = causal\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim_in, dim_inner * 3, bias = False)\n",
    "        self.to_out = nn.Linear(dim_inner, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if self.causal:\n",
    "            mask = torch.ones(sim.shape[-2:], device = device).triu(1).bool()\n",
    "            sim.masked_fill_(mask[None, ...], -torch.finfo(q.dtype).max)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        return self.to_out(out)\n",
    "\n",
    "class SpatialGatingUnit(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_seq,\n",
    "        causal = False,\n",
    "        act = nn.Identity(),\n",
    "        heads = 1,\n",
    "        init_eps = 1e-3,\n",
    "        circulant_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_out = dim // 2\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.norm = nn.LayerNorm(dim_out)\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        # parameters\n",
    "\n",
    "        if circulant_matrix:\n",
    "            self.circulant_pos_x = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "            self.circulant_pos_y = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "\n",
    "        self.circulant_matrix = circulant_matrix\n",
    "        shape = (heads, dim_seq,) if circulant_matrix else (heads, dim_seq, dim_seq)\n",
    "        weight = torch.zeros(shape)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        init_eps /= dim_seq\n",
    "        nn.init.uniform_(self.weight, -init_eps, init_eps)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "\n",
    "    def forward(self, x, gate_res = None):\n",
    "        device, n, h = x.device, x.shape[1], self.heads\n",
    "\n",
    "        res, gate = x.chunk(2, dim = -1)\n",
    "        gate = self.norm(gate)\n",
    "\n",
    "        weight, bias = self.weight, self.bias\n",
    "\n",
    "        if self.circulant_matrix:\n",
    "            # build the circulant matrix\n",
    "\n",
    "            dim_seq = weight.shape[-1]\n",
    "            weight = F.pad(weight, (0, dim_seq), value = 0)\n",
    "            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n",
    "            weight = weight[:, :-dim_seq].reshape(h, dim_seq, 2 * dim_seq - 1)\n",
    "            weight = weight[:, :, (dim_seq - 1):]\n",
    "\n",
    "            # give circulant matrix absolute position awareness\n",
    "\n",
    "            pos_x, pos_y = self.circulant_pos_x, self.circulant_pos_y\n",
    "            weight = weight * rearrange(pos_x, 'h i -> h i ()') * rearrange(pos_y, 'h j -> h () j')\n",
    "\n",
    "        if self.causal:\n",
    "            weight, bias = weight[:, :n, :n], bias[:, :n]\n",
    "            mask = torch.ones(weight.shape[-2:], device = device).triu_(1).bool()\n",
    "            mask = rearrange(mask, 'i j -> () i j')\n",
    "            weight = weight.masked_fill(mask, 0.)\n",
    "\n",
    "        gate = rearrange(gate, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        gate = einsum('b h n d, h m n -> b h m d', gate, weight)\n",
    "        gate = gate + rearrange(bias, 'h n -> () h n ()')\n",
    "\n",
    "        gate = rearrange(gate, 'b h n d -> b n (h d)')\n",
    "\n",
    "        if exists(gate_res):\n",
    "            gate = gate + gate_res\n",
    "\n",
    "        return self.act(gate) * res\n",
    "\n",
    "class gMLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_ff,\n",
    "        seq_len,\n",
    "        heads = 1,\n",
    "        attn_dim = None,\n",
    "        causal = False,\n",
    "        act = nn.Identity(),\n",
    "        circulant_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Sequential(\n",
    "            nn.Linear(dim, dim_ff),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.attn = Attention(dim, dim_ff // 2, attn_dim, causal) if exists(attn_dim) else None\n",
    "\n",
    "        self.sgu = SpatialGatingUnit(dim_ff, seq_len, causal, act, heads, circulant_matrix = circulant_matrix)\n",
    "        self.proj_out = nn.Linear(dim_ff // 2, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_res = self.attn(x) if exists(self.attn) else None\n",
    "        x = self.proj_in(x)\n",
    "        x = self.sgu(x, gate_res = gate_res)\n",
    "        x = self.proj_out(x)\n",
    "        return x\n",
    "\n",
    "# main classes\n",
    "\n",
    "class gMLPVision(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        image_size,\n",
    "        patch_size,\n",
    "        num_classes,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads = 1,\n",
    "        ff_mult = 4,\n",
    "        channels = 3,\n",
    "        attn_dim = None,\n",
    "        prob_survival = 1.\n",
    "    ):\n",
    "        super().__init__()\n",
    "        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert (image_height % patch_height) == 0 and (image_width % patch_width) == 0, 'image height and width must be divisible by patch size'\n",
    "        num_patches = (image_height // patch_height) * (image_width // patch_width)\n",
    "\n",
    "        dim_ff = dim * ff_mult\n",
    "\n",
    "        self.to_patch_embed = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)', p1 = patch_height, p2 = patch_width),\n",
    "            nn.Linear(channels * patch_height * patch_width, dim)\n",
    "        )\n",
    "\n",
    "        self.prob_survival = prob_survival\n",
    "\n",
    "        self.layers = nn.ModuleList([Residual(PreNorm(dim, gMLPBlock(dim = dim, heads = heads, dim_ff = dim_ff, seq_len = num_patches, attn_dim = attn_dim))) for i in range(depth)])\n",
    "\n",
    "        self.to_logits = nn.Sequential(\n",
    "            nn.LayerNorm(dim),\n",
    "            Reduce('b n d -> b d', 'mean'),\n",
    "            nn.Linear(dim, num_classes)\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embed(x)\n",
    "        layers = self.layers if not self.training else dropout_layers(self.layers, self.prob_survival)\n",
    "        x = nn.Sequential(*layers)(x)\n",
    "        return self.to_logits(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attention"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Attention(nn.Module):\n",
    "    def __init__(self, dim_in, dim_out, dim_inner, causal = False):\n",
    "        super().__init__()\n",
    "        self.scale = dim_inner ** -0.5\n",
    "        self.causal = causal\n",
    "\n",
    "        self.to_qkv = nn.Linear(dim_in, dim_inner * 3, bias = False)\n",
    "        self.to_out = nn.Linear(dim_inner, dim_out)\n",
    "\n",
    "    def forward(self, x):\n",
    "        device = x.device\n",
    "        q, k, v = self.to_qkv(x).chunk(3, dim = -1)\n",
    "        sim = einsum('b i d, b j d -> b i j', q, k) * self.scale\n",
    "\n",
    "        if self.causal:\n",
    "            mask = torch.ones(sim.shape[-2:], device = device).triu(1).bool()\n",
    "            sim.masked_fill_(mask[None, ...], -torch.finfo(q.dtype).max)\n",
    "\n",
    "        attn = sim.softmax(dim = -1)\n",
    "        out = einsum('b i j, b j d -> b i d', attn, v)\n",
    "        return self.to_out(out)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SpatialGatingUnit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SpatialGatingUnit(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        dim_seq,\n",
    "        causal = False,\n",
    "        act = nn.Identity(),\n",
    "        heads = 1,\n",
    "        init_eps = 1e-3,\n",
    "        circulant_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        dim_out = dim // 2\n",
    "        self.heads = heads\n",
    "        self.causal = causal\n",
    "        self.norm = nn.LayerNorm(dim_out)\n",
    "\n",
    "        self.act = act\n",
    "\n",
    "        # parameters\n",
    "\n",
    "        if circulant_matrix:\n",
    "            self.circulant_pos_x = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "            self.circulant_pos_y = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "\n",
    "        self.circulant_matrix = circulant_matrix\n",
    "        shape = (heads, dim_seq,) if circulant_matrix else (heads, dim_seq, dim_seq)\n",
    "        weight = torch.zeros(shape)\n",
    "\n",
    "        self.weight = nn.Parameter(weight)\n",
    "        init_eps /= dim_seq\n",
    "        nn.init.uniform_(self.weight, -init_eps, init_eps)\n",
    "\n",
    "        self.bias = nn.Parameter(torch.ones(heads, dim_seq))\n",
    "\n",
    "    def forward(self, x, gate_res = None):\n",
    "        device, n, h = x.device, x.shape[1], self.heads\n",
    "\n",
    "        res, gate = x.chunk(2, dim = -1)\n",
    "        gate = self.norm(gate)\n",
    "\n",
    "        weight, bias = self.weight, self.bias\n",
    "\n",
    "        if self.circulant_matrix:\n",
    "            # build the circulant matrix\n",
    "\n",
    "            dim_seq = weight.shape[-1]\n",
    "            weight = F.pad(weight, (0, dim_seq), value = 0)\n",
    "            weight = repeat(weight, '... n -> ... (r n)', r = dim_seq)\n",
    "            weight = weight[:, :-dim_seq].reshape(h, dim_seq, 2 * dim_seq - 1)\n",
    "            weight = weight[:, :, (dim_seq - 1):]\n",
    "\n",
    "            # give circulant matrix absolute position awareness\n",
    "\n",
    "            pos_x, pos_y = self.circulant_pos_x, self.circulant_pos_y\n",
    "            weight = weight * rearrange(pos_x, 'h i -> h i ()') * rearrange(pos_y, 'h j -> h () j')\n",
    "\n",
    "        if self.causal:\n",
    "            weight, bias = weight[:, :n, :n], bias[:, :n]\n",
    "            mask = torch.ones(weight.shape[-2:], device = device).triu_(1).bool()\n",
    "            mask = rearrange(mask, 'i j -> () i j')\n",
    "            weight = weight.masked_fill(mask, 0.)\n",
    "\n",
    "        gate = rearrange(gate, 'b n (h d) -> b h n d', h = h)\n",
    "\n",
    "        gate = einsum('b h n d, h m n -> b h m d', gate, weight)\n",
    "        gate = gate + rearrange(bias, 'h n -> () h n ()')\n",
    "\n",
    "        gate = rearrange(gate, 'b h n d -> b n (h d)')\n",
    "\n",
    "        if exists(gate_res):\n",
    "            gate = gate + gate_res\n",
    "\n",
    "        return self.act(gate) * res\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# gMLPBlock"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gMLPBlock(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        *,\n",
    "        dim,\n",
    "        dim_ff,\n",
    "        seq_len,\n",
    "        heads = 1,\n",
    "        attn_dim = None,\n",
    "        causal = False,\n",
    "        act = nn.Identity(),\n",
    "        circulant_matrix = False\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.proj_in = nn.Sequential(\n",
    "            nn.Linear(dim, dim_ff),\n",
    "            nn.GELU()\n",
    "        )\n",
    "\n",
    "        self.attn = Attention(dim, dim_ff // 2, attn_dim, causal) if exists(attn_dim) else None\n",
    "\n",
    "        self.sgu = SpatialGatingUnit(dim_ff, seq_len, causal, act, heads, circulant_matrix = circulant_matrix)\n",
    "        self.proj_out = nn.Linear(dim_ff // 2, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        gate_res = self.attn(x) if exists(self.attn) else None\n",
    "        x = self.proj_in(x)\n",
    "        x = self.sgu(x, gate_res = gate_res)\n",
    "        x = self.proj_out(x)\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class gMLPVision(nn.Module):\n",
    "    def __init__(self,\n",
    "                 *,\n",
    "                 image_size,\n",
    "                 patch_size,\n",
    "                 num_classes,\n",
    "                 dim,\n",
    "                 depth,\n",
    "                 heads=1,\n",
    "                 ff_mult=4,\n",
    "                 channels=3,\n",
    "                 attn_dim=None,\n",
    "                 prob_survival=1.):\n",
    "        super().__init__()\n",
    "        assert (dim % heads) == 0, 'dimension must be divisible by number of heads'\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "        assert (image_height % patch_height) == 0 and (\n",
    "            image_width % patch_width\n",
    "        ) == 0, 'image height and width must be divisible by patch size'\n",
    "        num_patches = (image_height // patch_height) * (image_width //\n",
    "                                                        patch_width)\n",
    "\n",
    "        dim_ff = dim * ff_mult\n",
    "\n",
    "        self.to_patch_embed = nn.Sequential(\n",
    "            Rearrange('b c (h p1) (w p2) -> b (h w) (c p1 p2)',\n",
    "                      p1=patch_height,\n",
    "                      p2=patch_width),\n",
    "            nn.Linear(channels * patch_height * patch_width, dim))\n",
    "\n",
    "        self.prob_survival = prob_survival\n",
    "\n",
    "        self.layers = nn.ModuleList([\n",
    "            #Residual 相当与添加了残差链接\n",
    "            Residual(\n",
    "                PreNorm(\n",
    "                    dim,\n",
    "                    gMLPBlock(dim=dim,\n",
    "                              heads=heads,\n",
    "                              dim_ff=dim_ff,\n",
    "                              seq_len=num_patches,\n",
    "                              attn_dim=attn_dim))) for i in range(depth)\n",
    "        ])\n",
    "\n",
    "        self.to_logits = nn.Sequential(nn.LayerNorm(dim),\n",
    "                                       Reduce('b n d -> b d', 'mean'),\n",
    "                                       nn.Linear(dim, num_classes))\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.to_patch_embed(x)\n",
    "        layers = self.layers if not self.training else dropout_layers(\n",
    "            self.layers, self.prob_survival)\n",
    "        x = nn.Sequential(*layers)(x)\n",
    "        return self.to_logits(x)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
