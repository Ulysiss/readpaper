{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 图片来源\n",
    "https://github.com/lucidrains/vit-pytorch\n",
    "# 代码来源\n",
    "https://github.com/gupta-abhay/pytorch-vit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "\n",
    "class PreNorm(nn.Module):\n",
    "    def __init__(self, dim, fn):\n",
    "        super(PreNorm, self).__init__()\n",
    "        self.norm = nn.LayerNorm(dim)\n",
    "        self.fn = fn\n",
    "\n",
    "    def forward(self, x, **kwargs):\n",
    "        return self.fn(self.norm(x), **kwargs)\n",
    "\n",
    "\n",
    "class Attention(nn.Module):\n",
    "    def __init__(\n",
    "        self, dim, num_heads=8, qkv_bias=False, attn_drop=0.0, proj_drop=0.0\n",
    "    ):\n",
    "        super(Attention, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            dim % num_heads == 0\n",
    "        ), \"Embedding dimension should be divisible by number of heads\"\n",
    "\n",
    "        self.num_heads = num_heads\n",
    "        head_dim = dim // num_heads\n",
    "        self.scale = head_dim ** -0.5\n",
    "\n",
    "        self.qkv = nn.Linear(dim, dim * 3, bias=qkv_bias)\n",
    "        self.attn_drop = nn.Dropout(attn_drop)\n",
    "        self.proj = nn.Linear(dim, dim)\n",
    "        self.proj_drop = nn.Dropout(proj_drop)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, N, C = x.shape\n",
    "        qkv = (\n",
    "            self.qkv(x)\n",
    "            .reshape(B, N, 3, self.num_heads, C // self.num_heads)\n",
    "            .permute(2, 0, 3, 1, 4)\n",
    "        )\n",
    "        # make torchscript happy (cannot use tensor as tuple)\n",
    "        q, k, v = qkv[0], qkv[1], qkv[2]\n",
    "\n",
    "        attn = (q @ k.transpose(-2, -1)) * self.scale\n",
    "        attn = attn.softmax(dim=-1)\n",
    "        attn = self.attn_drop(attn)\n",
    "\n",
    "        x = (attn @ v).transpose(1, 2).reshape(B, N, C)\n",
    "        x = self.proj(x)\n",
    "        x = self.proj_drop(x)\n",
    "        return x\n",
    "\n",
    "\n",
    "class FeedForward(nn.Module):\n",
    "    \"\"\"\n",
    "    Implementation of MLP for transformer\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, dim, hidden_dim, dropout_rate=0.0, revised=False):\n",
    "        super(FeedForward, self).__init__()\n",
    "        if not revised:\n",
    "            \"\"\"\n",
    "            Original: https://arxiv.org/pdf/2010.11929.pdf\n",
    "            \"\"\"\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Linear(dim, hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.Linear(hidden_dim, dim),\n",
    "            )\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Scaled ReLU: https://arxiv.org/pdf/2109.03810.pdf\n",
    "            \"\"\"\n",
    "            self.net = nn.Sequential(\n",
    "                nn.Conv1d(dim, hidden_dim, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm1d(hidden_dim),\n",
    "                nn.GELU(),\n",
    "                nn.Dropout(p=dropout_rate),\n",
    "                nn.Conv1d(hidden_dim, dim, kernel_size=1, stride=1),\n",
    "                nn.BatchNorm1d(dim),\n",
    "                nn.GELU(),\n",
    "            )\n",
    "\n",
    "        self.revised = revised\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, module in self.net.named_children():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                nn.init.normal_(module.bias, std=1e-6)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.revised:\n",
    "            x = x.permute(0, 2, 1)\n",
    "            x = self.net(x)\n",
    "            x = x.permute(0, 2, 1)\n",
    "        else:\n",
    "            x = self.net(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "\n",
    "class OutputLayer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        embedding_dim,\n",
    "        num_classes=1000,\n",
    "        representation_size=None,\n",
    "        cls_head=False,\n",
    "    ):\n",
    "        super(OutputLayer, self).__init__()\n",
    "\n",
    "        self.num_classes = num_classes\n",
    "        modules = []\n",
    "        if representation_size:\n",
    "            modules.append(nn.Linear(embedding_dim, representation_size))\n",
    "            modules.append(nn.Tanh())\n",
    "            modules.append(nn.Linear(representation_size, num_classes))\n",
    "        else:\n",
    "            modules.append(nn.Linear(embedding_dim, num_classes))\n",
    "\n",
    "        self.net = nn.Sequential(*modules)\n",
    "\n",
    "        if cls_head:\n",
    "            self.to_cls_token = nn.Identity()\n",
    "\n",
    "        self.cls_head = cls_head\n",
    "        self.num_classes = num_classes\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        for name, module in self.net.named_children():\n",
    "            if isinstance(module, nn.Linear):\n",
    "                if module.weight.shape[0] == self.num_classes:\n",
    "                    nn.init.zeros_(module.weight)\n",
    "                    nn.init.zeros_(module.bias)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.cls_head:\n",
    "            x = self.to_cls_token(x[:, 0])\n",
    "        else:\n",
    "            \"\"\"\n",
    "            Scaling Vision Transformer: https://arxiv.org/abs/2106.04560\n",
    "            \"\"\"\n",
    "            x = torch.mean(x, dim=1)\n",
    "\n",
    "        return self.net(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# patch_embed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from einops.layers.torch import Rearrange\n",
    "from utils import trunc_normal_\n",
    "\n",
    "\n",
    "def pair(t):\n",
    "    return t if isinstance(t, tuple) else (t, t)\n",
    "\n",
    "\n",
    "class EmbeddingStem(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        channels=3,\n",
    "        embedding_dim=768,\n",
    "        hidden_dims=None,\n",
    "        conv_patch=False,\n",
    "        linear_patch=False,\n",
    "        conv_stem=True,\n",
    "        conv_stem_original=True,\n",
    "        conv_stem_scaled_relu=False,\n",
    "        position_embedding_dropout=None,\n",
    "        cls_head=True,\n",
    "    ):\n",
    "        super(EmbeddingStem, self).__init__()\n",
    "\n",
    "        assert (\n",
    "            sum([conv_patch, conv_stem, linear_patch]) == 1\n",
    "        ), \"Only one of three modes should be active\"\n",
    "\n",
    "        image_height, image_width = pair(image_size)\n",
    "        patch_height, patch_width = pair(patch_size)\n",
    "\n",
    "        assert (\n",
    "            image_height % patch_height == 0 and image_width % patch_width == 0\n",
    "        ), \"Image dimensions must be divisible by the patch size.\"\n",
    "\n",
    "        assert not (\n",
    "            conv_stem and cls_head\n",
    "        ), \"Cannot use [CLS] token approach with full conv stems for ViT\"\n",
    "\n",
    "        if linear_patch or conv_patch:\n",
    "            self.grid_size = (\n",
    "                image_height // patch_height,\n",
    "                image_width // patch_width,\n",
    "            )\n",
    "            num_patches = self.grid_size[0] * self.grid_size[1]\n",
    "\n",
    "            if cls_head:\n",
    "                self.cls_token = nn.Parameter(torch.zeros(1, 1, embedding_dim))\n",
    "                num_patches += 1\n",
    "\n",
    "            # positional embedding\n",
    "            self.pos_embed = nn.Parameter(\n",
    "                torch.zeros(1, num_patches, embedding_dim)\n",
    "            )\n",
    "            self.pos_drop = nn.Dropout(p=position_embedding_dropout)\n",
    "\n",
    "        if conv_patch:\n",
    "            self.projection = nn.Sequential(\n",
    "                nn.Conv2d(\n",
    "                    channels,\n",
    "                    embedding_dim,\n",
    "                    kernel_size=patch_size,\n",
    "                    stride=patch_size,\n",
    "                ),\n",
    "            )\n",
    "        elif linear_patch:\n",
    "            patch_dim = channels * patch_height * patch_width\n",
    "            self.projection = nn.Sequential(\n",
    "                Rearrange(\n",
    "                    'b c (h p1) (w p2) -> b (h w) (p1 p2 c)',\n",
    "                    p1=patch_height,\n",
    "                    p2=patch_width,\n",
    "                ),\n",
    "                nn.Linear(patch_dim, embedding_dim),\n",
    "            )\n",
    "        elif conv_stem:\n",
    "            assert (\n",
    "                conv_stem_scaled_relu ^ conv_stem_original\n",
    "            ), \"Can use either the original or the scaled relu stem\"\n",
    "\n",
    "            if not isinstance(hidden_dims, list):\n",
    "                raise ValueError(\"Cannot create stem without list of sizes\")\n",
    "\n",
    "            if conv_stem_original:\n",
    "                \"\"\"\n",
    "                Conv stem from https://arxiv.org/pdf/2106.14881.pdf\n",
    "                \"\"\"\n",
    "\n",
    "                hidden_dims.insert(0, channels)\n",
    "                modules = []\n",
    "                for i, (in_ch, out_ch) in enumerate(\n",
    "                    zip(hidden_dims[:-1], hidden_dims[1:])\n",
    "                ):\n",
    "                    modules.append(\n",
    "                        nn.Conv2d(\n",
    "                            in_ch,\n",
    "                            out_ch,\n",
    "                            kernel_size=3,\n",
    "                            stride=2 if in_ch != out_ch else 1,\n",
    "                            padding=1,\n",
    "                            bias=False,\n",
    "                        ),\n",
    "                    )\n",
    "                    modules.append(nn.BatchNorm2d(out_ch),)\n",
    "                    modules.append(nn.ReLU(inplace=True))\n",
    "\n",
    "                modules.append(\n",
    "                    nn.Conv2d(\n",
    "                        hidden_dims[-1], embedding_dim, kernel_size=1, stride=1,\n",
    "                    ),\n",
    "                )\n",
    "                self.projection = nn.Sequential(*modules)\n",
    "\n",
    "            elif conv_stem_scaled_relu:\n",
    "                \"\"\"\n",
    "                Conv stem from https://arxiv.org/pdf/2109.03810.pdf\n",
    "                \"\"\"\n",
    "                assert (\n",
    "                    len(hidden_dims) == 1\n",
    "                ), \"Only one value for hidden_dim is allowed\"\n",
    "                mid_ch = hidden_dims[0]\n",
    "\n",
    "                # fmt: off\n",
    "                self.projection = nn.Sequential(\n",
    "                    nn.Conv2d(\n",
    "                        channels, mid_ch,\n",
    "                        kernel_size=7, stride=2, padding=3, bias=False,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(mid_ch),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(\n",
    "                        mid_ch, mid_ch,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(mid_ch),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(\n",
    "                        mid_ch, mid_ch,\n",
    "                        kernel_size=3, stride=1, padding=1, bias=False,\n",
    "                    ),\n",
    "                    nn.BatchNorm2d(mid_ch),\n",
    "                    nn.ReLU(inplace=True),\n",
    "                    nn.Conv2d(\n",
    "                        mid_ch, embedding_dim,\n",
    "                        kernel_size=patch_size // 2, stride=patch_size // 2,\n",
    "                    ),\n",
    "                )\n",
    "                # fmt: on\n",
    "\n",
    "            else:\n",
    "                raise ValueError(\"Undefined convolutional stem type defined\")\n",
    "\n",
    "        self.conv_stem = conv_stem\n",
    "        self.conv_patch = conv_patch\n",
    "        self.linear_patch = linear_patch\n",
    "        self.cls_head = cls_head\n",
    "\n",
    "        self._init_weights()\n",
    "\n",
    "    def _init_weights(self):\n",
    "        if not self.conv_stem:\n",
    "            trunc_normal_(self.pos_embed, std=0.02)\n",
    "\n",
    "    def forward(self, x):\n",
    "        if self.conv_stem:\n",
    "            x = self.projection(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "            return x\n",
    "\n",
    "        # paths for cls_token / position embedding\n",
    "        elif self.linear_patch:\n",
    "            x = self.projection(x)\n",
    "        elif self.conv_patch:\n",
    "            x = self.projection(x)\n",
    "            x = x.flatten(2).transpose(1, 2)\n",
    "\n",
    "        if self.cls_head:\n",
    "            cls_token = self.cls_token.expand(x.shape[0], -1, -1)\n",
    "            x = torch.cat((cls_token, x), dim=1)\n",
    "        return self.pos_drop(x + self.pos_embed)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# transformer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\transformerencoder.png width=\"40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from modules import Attention, FeedForward, PreNorm\n",
    "\n",
    "\n",
    "class Transformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        dim,\n",
    "        depth,\n",
    "        heads,\n",
    "        mlp_ratio=4.0,\n",
    "        attn_dropout=0.0,\n",
    "        dropout=0.0,\n",
    "        qkv_bias=True,\n",
    "        revised=False,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.layers = nn.ModuleList([])\n",
    "\n",
    "        assert isinstance(\n",
    "            mlp_ratio, float\n",
    "        ), \"MLP ratio should be an integer for valid \"\n",
    "        mlp_dim = int(mlp_ratio * dim)\n",
    "\n",
    "        for _ in range(depth):\n",
    "            self.layers.append(\n",
    "                nn.ModuleList(\n",
    "                    [\n",
    "                        PreNorm(\n",
    "                            dim,\n",
    "                            Attention(\n",
    "                                dim,\n",
    "                                num_heads=heads,\n",
    "                                qkv_bias=qkv_bias,\n",
    "                                attn_drop=attn_dropout,\n",
    "                                proj_drop=dropout,\n",
    "                            ),\n",
    "                        ),\n",
    "                        PreNorm(\n",
    "                            dim,\n",
    "                            FeedForward(dim, mlp_dim, dropout_rate=dropout,),\n",
    "                        )\n",
    "                        if not revised\n",
    "                        else FeedForward(\n",
    "                            dim, mlp_dim, dropout_rate=dropout, revised=True,\n",
    "                        ),\n",
    "                    ]\n",
    "                )\n",
    "            )\n",
    "\n",
    "    def forward(self, x):\n",
    "        for attn, ff in self.layers:\n",
    "            x = attn(x) + x\n",
    "            x = ff(x) + x\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div align=center>\n",
    "<img src=.\\img\\vit.gif width=\"40%\"/>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn as nn\n",
    "\n",
    "from patch_embed import EmbeddingStem\n",
    "from transformer import Transformer\n",
    "from modules import OutputLayer\n",
    "\n",
    "\n",
    "class VisionTransformer(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        image_size=224,\n",
    "        patch_size=16,\n",
    "        in_channels=3,\n",
    "        embedding_dim=768,\n",
    "        num_layers=12,\n",
    "        num_heads=12,\n",
    "        qkv_bias=True,\n",
    "        mlp_ratio=4.0,\n",
    "        use_revised_ffn=False,\n",
    "        dropout_rate=0.0,\n",
    "        attn_dropout_rate=0.0,\n",
    "        use_conv_stem=True,\n",
    "        use_conv_patch=False,\n",
    "        use_linear_patch=False,\n",
    "        use_conv_stem_original=True,\n",
    "        use_stem_scaled_relu=False,\n",
    "        hidden_dims=None,\n",
    "        cls_head=False,\n",
    "        num_classes=1000,\n",
    "        representation_size=None,\n",
    "    ):\n",
    "        super(VisionTransformer, self).__init__()\n",
    "\n",
    "        # embedding layer\n",
    "        self.embedding_layer = EmbeddingStem(\n",
    "            image_size=image_size,\n",
    "            patch_size=patch_size,\n",
    "            channels=in_channels,\n",
    "            embedding_dim=embedding_dim,\n",
    "            hidden_dims=hidden_dims,\n",
    "            conv_patch=use_conv_patch,\n",
    "            linear_patch=use_linear_patch,\n",
    "            conv_stem=use_conv_stem,\n",
    "            conv_stem_original=use_conv_stem_original,\n",
    "            conv_stem_scaled_relu=use_stem_scaled_relu,\n",
    "            position_embedding_dropout=dropout_rate,\n",
    "            cls_head=cls_head,\n",
    "        )\n",
    "\n",
    "        # transformer\n",
    "        self.transformer = Transformer(\n",
    "            dim=embedding_dim,\n",
    "            depth=num_layers,\n",
    "            heads=num_heads,\n",
    "            mlp_ratio=mlp_ratio,\n",
    "            attn_dropout=attn_dropout_rate,\n",
    "            dropout=dropout_rate,\n",
    "            qkv_bias=qkv_bias,\n",
    "            revised=use_revised_ffn,\n",
    "        )\n",
    "        self.post_transformer_ln = nn.LayerNorm(embedding_dim)\n",
    "\n",
    "        # output layer\n",
    "        self.cls_layer = OutputLayer(\n",
    "            embedding_dim,\n",
    "            num_classes=num_classes,\n",
    "            representation_size=representation_size,\n",
    "            cls_head=cls_head,\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.embedding_layer(x)\n",
    "        x = self.transformer(x)\n",
    "        x = self.post_transformer_ln(x)\n",
    "        x = self.cls_layer(x)\n",
    "        return x"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.8 ('base')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.8.8"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "5179d32cf6ec497baf3f8a3ef987cc77c5d2dc691fdde20a56316522f61a7323"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
